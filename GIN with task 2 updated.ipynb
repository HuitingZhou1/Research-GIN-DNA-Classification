{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":699,"status":"ok","timestamp":1718933882888,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"},"user_tz":300},"id":"4N7yni47SeHB","outputId":"d4a646ec-1bea-4238-b404-b5a3709841df"},"outputs":[{"output_type":"stream","name":"stdout","text":["chr2L node labels shape: (20000, 21)\n","chr2L adjacency matrix shape: (20000, 442)\n","chr2R node labels shape: (20000, 21)\n","chr2R adjacency matrix shape: (20000, 442)\n","chr3L node labels shape: (20000, 21)\n","chr3L adjacency matrix shape: (20000, 442)\n","chr3R node labels shape: (20000, 21)\n","chr3R adjacency matrix shape: (20000, 442)\n"]}],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Aug 23 12:22:38 2023\n","\n","@author: vishalr\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","# change this to the folder where you store your data\n","data_dir = r\"/content/sample_data/\"\n","\n","# each of the two data frames below have 20,000 rows, each corresponding to one sample from the original graph\n","# each sample consists of 21 nodes; node_labels contains the names of these 21 nodes of the form Vxyz\n","# each node is a DNA fragment of length 500 bases; so Vxyz coveres region [500*xyz, 500*xyz+500)\n","# adjacency_matrix has the flattended adjacecny matrix for each of these 20,000 samples\n","# so each row is of dimension 21*21 = 441\n","\n","# node_labels = pd.read_pickle(data_dir+'df_chr2L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot_sample_node_matrix')\n","# adjacency_matrix = pd.read_pickle(data_dir+'df_chr2L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot')\n","\n","node_labels_chr2L = np.load(data_dir + 'df_chr2L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot_sample_node_matrix.npy')\n","adjacency_matrix_chr2L = np.load(data_dir + 'df_chr2L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","\n","node_labels_chr2R = np.load(data_dir + 'df_chr2R_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot_sample_node_matrix.npy')\n","adjacency_matrix_chr2R = np.load(data_dir + 'df_chr2R_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","\n","node_labels_chr3L = np.load(data_dir + 'df_chr3L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot_sample_node_matrix.npy')\n","adjacency_matrix_chr3L = np.load(data_dir + 'df_chr3L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","\n","node_labels_chr3R = np.load(data_dir + 'df_chr3R_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot_sample_node_matrix.npy')\n","adjacency_matrix_chr3R = np.load(data_dir + 'df_chr3R_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","\n","# just seeing the size of input; doesn't really do anything\n","print(\"chr2L node labels shape:\", node_labels_chr2L.shape)\n","print(\"chr2L adjacency matrix shape:\", adjacency_matrix_chr2L.shape)\n","print(\"chr2R node labels shape:\", node_labels_chr2R.shape)\n","print(\"chr2R adjacency matrix shape:\", adjacency_matrix_chr2R.shape)\n","print(\"chr3L node labels shape:\", node_labels_chr3L.shape)\n","print(\"chr3L adjacency matrix shape:\", adjacency_matrix_chr3L.shape)\n","print(\"chr3R node labels shape:\", node_labels_chr3R.shape)\n","print(\"chr3R adjacency matrix shape:\", adjacency_matrix_chr3R.shape)\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"nkCP8LwZrSVY","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"status":"error","timestamp":1718330957744,"user_tz":300,"elapsed":6349,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"outputId":"b92738a7-3a3f-46cf-88b5-8e7b48ef2a76"},"execution_count":null,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34432,"status":"ok","timestamp":1718933919735,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"},"user_tz":300},"id":"_EM4mJRqWvkI","outputId":"14642ecf-5837-4eb3-dbb0-02aedb2f607e"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# We assume that PyTorch is already installed\n","import torch\n","torchversion = torch.__version__\n","\n","# Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","\n","# Visualization\n","import networkx as nx\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1718401321427,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"},"user_tz":300},"id":"T3UtXcb7Dl8m","outputId":"ee040c12-bca1-4175-ecfe-ac96be2112d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 20000\n"]}],"source":["# trying print the shape of the loaded array to get the number of samples\n","print(f'Number of samples: {adjacency_matrix_chr2L.shape[0]}')"]},{"cell_type":"code","source":["#part1a: delete repeated node in adjacency matrix\n","import numpy as np\n","\n","def remove_duplicates(node_labels, adj_matrix):\n","    unique_nodes = []\n","    unique_indices = []\n","\n","    for idx, node in enumerate(node_labels):\n","        if node not in unique_nodes:\n","            unique_nodes.append(node)\n","            unique_indices.append(idx)\n","\n","    unique_nodes = np.array(unique_nodes)\n","    sample_adj_matrix = adj_matrix[np.ix_(unique_indices, unique_indices)]\n","\n","    return unique_nodes, sample_adj_matrix\n","\n","# test whether it is functioning\n","sample_node_labels = np.array([1, 2, 2, 3, 4, 4])\n","sample_adj_matrix = np.array([\n","    [0, 1, 1, 0, 0, 0],\n","    [1, 0, 1, 0, 0, 0],\n","    [1, 1, 0, 1, 0, 0],\n","    [0, 0, 1, 0, 1, 1],\n","    [0, 0, 0, 1, 0, 1],\n","    [0, 0, 0, 1, 1, 0]\n","])\n","\n","cleaned_nodes, cleaned_adj_matrix = remove_duplicates(sample_node_labels, sample_adj_matrix)\n","\n","print(\"Cleaned Nodes:\", cleaned_nodes)\n","print(\"Cleaned Adjacency Matrix:\\n\", cleaned_adj_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QuXEba2k1dmG","executionInfo":{"status":"ok","timestamp":1718933919736,"user_tz":300,"elapsed":9,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"outputId":"797d3c61-18e1-4a14-f252-4f25a26960a4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned Nodes: [1 2 3 4]\n","Cleaned Adjacency Matrix:\n"," [[0 1 0 0]\n"," [1 0 0 0]\n"," [0 0 0 1]\n"," [0 0 1 0]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the total length of the array\n","total_length = 240000\n","\n","# Create an array with values from 0 to 239999\n","all_node_indices = np.arange(total_length)\n","\n","# Print to check the result\n","print(\"All node indices array:\", all_node_indices)\n","print(\"Shape of all node indices array:\", all_node_indices.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718937179044,"user_tz":300,"elapsed":182,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"outputId":"ca633a02-b072-4947-a2e4-5d0fde2fa08a","id":"uyL1YiQVpr4_"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["All node indices array: [     0      1      2 ... 239997 239998 239999]\n","Shape of all node indices array: (240000,)\n"]}]},{"cell_type":"code","source":["# part2b：indicate nodes' class\n","file_indices = []\n","\n","section_length = 60000\n","num_sections = 4\n","\n","for section in range(num_sections):\n","    file_indices.extend([section] * section_length)\n","\n","file_indices = np.array(file_indices)\n","\n","# Print to check the result\n","print(\"File indices array:\", file_indices)\n","print(\"Shape of file indices array:\", file_indices.shape)\n","\n","# Print each section to verify\n","print(\"First section (should be 0):\")\n","print(file_indices[:section_length])\n","\n","print(\"Second section (should be 1):\")\n","print(file_indices[section_length:2 * section_length])\n","\n","print(\"Third section (should be 2):\")\n","print(file_indices[2 * section_length:3 * section_length])\n","\n","print(\"Fourth section (should be 3):\")\n","print(file_indices[3 * section_length:])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B73Dkaskp1Ey","executionInfo":{"status":"ok","timestamp":1718401321428,"user_tz":300,"elapsed":4,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"outputId":"563c5c5c-d2de-4e91-f8f9-20ea2aae2b56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File indices array: [0 0 0 ... 3 3 3]\n","Shape of file indices array: (240000,)\n","First section (should be 0):\n","[0 0 0 ... 0 0 0]\n","Second section (should be 1):\n","[1 1 1 ... 1 1 1]\n","Third section (should be 2):\n","[2 2 2 ... 2 2 2]\n","Fourth section (should be 3):\n","[3 3 3 ... 3 3 3]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"s9RKTk5Biztt"}},{"cell_type":"code","source":["#part 2c: edge list\n","import numpy as np\n","import scipy.sparse as sp\n","\n","def extract_edges_from_subgraph(adjacency_matrix):\n","    num_nodes = 21\n","    edges = []\n","\n","    for i in range(adjacency_matrix.shape[0]):\n","        # Reshape the adjacency matrix\n","        adj_matrix = adjacency_matrix[i, :num_nodes*num_nodes].reshape((num_nodes, num_nodes))\n","\n","        for row in range(num_nodes):\n","            for col in range(row + 1, num_nodes):\n","                if adj_matrix[row, col] == 1:  # If there is an edge\n","                    edges.append(np.array([row, col]))\n","\n","    return edges\n","\n","# Extract edges from the adjacency matrices\n","edges_chr2L = extract_edges_from_subgraph(adjacency_matrix_chr2L)\n","edges_chr2R = extract_edges_from_subgraph(adjacency_matrix_chr2R)\n","edges_chr3L = extract_edges_from_subgraph(adjacency_matrix_chr3L)\n","edges_chr3R = extract_edges_from_subgraph(adjacency_matrix_chr3R)\n","\n","# Print results to verify\n","print(\"Edges from chr2L:\", edges_chr2L[:10])  # Print the first 10 edges as an example\n","print(\"Total edges in chr2L:\", len(edges_chr2L))\n","print(\"Edges from chr2R:\", edges_chr2R[:10])\n","print(\"Total edges in chr2R:\", len(edges_chr2R))\n","print(\"Edges from chr3L:\", edges_chr3L[:10])\n","print(\"Total edges in chr3L:\", len(edges_chr3L))\n","print(\"Edges from chr3R:\", edges_chr3R[:10])\n","print(\"Total edges in chr3R:\", len(edges_chr3R))\n","\n","all_edges = edges_chr2L + edges_chr2R + edges_chr3L + edges_chr3R\n","\n","# Print the total number of edges\n","print(\"Total edges across all subgraphs:\", len(all_edges))\n","\n","# Save edges to a file\n","edges_file = 'edges.npy'\n","np.save(edges_file, all_edges)\n","\n","# Load edges from the file to verify\n","loaded_edges = np.load(edges_file, allow_pickle=True)\n","print(f\"Total edges loaded: {len(loaded_edges)}\")\n","print(f\"First 10 edges: {loaded_edges[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9heKfmBSglYJ","executionInfo":{"status":"ok","timestamp":1718940344670,"user_tz":300,"elapsed":10190,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"outputId":"007e020d-cae2-4954-ef66-696baa24837f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Edges from chr2L: [array([0, 1]), array([0, 2]), array([1, 2]), array([2, 3]), array([3, 4]), array([4, 5]), array([4, 7]), array([4, 9]), array([ 4, 11]), array([5, 6])]\n","Total edges in chr2L: 722875\n","Edges from chr2R: [array([0, 1]), array([1, 2]), array([1, 4]), array([1, 7]), array([1, 9]), array([2, 3]), array([2, 5]), array([2, 6]), array([2, 8]), array([ 2, 10])]\n","Total edges in chr2R: 754087\n","Edges from chr3L: [array([0, 1]), array([1, 2]), array([2, 3]), array([2, 5]), array([2, 6]), array([2, 8]), array([3, 4]), array([3, 6]), array([4, 5]), array([5, 6])]\n","Total edges in chr3L: 679556\n","Edges from chr3R: [array([0, 1]), array([1, 2]), array([1, 7]), array([1, 8]), array([2, 3]), array([2, 5]), array([2, 7]), array([3, 4]), array([3, 6]), array([3, 7])]\n","Total edges in chr3R: 718848\n","Total edges across all subgraphs: 2875366\n","Total edges loaded: 2875366\n","First 10 edges: [[ 0  1]\n"," [ 0  2]\n"," [ 1  2]\n"," [ 2  3]\n"," [ 3  4]\n"," [ 4  5]\n"," [ 4  7]\n"," [ 4  9]\n"," [ 4 11]\n"," [ 5  6]]\n"]}]},{"cell_type":"code","source":["# part 2d: relate edge to subgraph\n","import numpy as np\n","\n","def extract_edges_and_indices(adjacency_matrix, class_offset):\n","    num_nodes = 21\n","    edges = []\n","    subgraph_indices = []\n","\n","    for subgraph_idx in range(adjacency_matrix.shape[0]):\n","        adj_matrix = adjacency_matrix[subgraph_idx, :num_nodes*num_nodes].reshape((num_nodes, num_nodes))\n","\n","        for row in range(num_nodes):\n","            for col in range(row + 1, num_nodes):\n","                if adj_matrix[row, col] == 1:  # If there is an edge\n","                    edges.append(np.array([row, col]))\n","                    subgraph_indices.append(class_offset + subgraph_idx)  # Add class offset to subgraph index\n","\n","    return edges, subgraph_indices\n","\n","# Load the subgraph files\n","data_dir = '/content/sample_data/'\n","adjacency_matrix_chr2L = np.load(data_dir + 'df_chr2L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","adjacency_matrix_chr2R = np.load(data_dir + 'df_chr2R_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","adjacency_matrix_chr3L = np.load(data_dir + 'df_chr3L_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","adjacency_matrix_chr3R = np.load(data_dir + 'df_chr3R_drosophila_ChIA_Drop_0.1_PASS_20000_MCMC_pivot.npy')\n","\n","# Define offsets for each class\n","class_offsets = [0, 20000, 40000, 60000]\n","\n","# Extract edges and subgraph indices from the subgraph files\n","edges_chr2L, subgraph_indices_chr2L = extract_edges_and_indices(adjacency_matrix_chr2L, class_offsets[0])\n","edges_chr2R, subgraph_indices_chr2R = extract_edges_and_indices(adjacency_matrix_chr2R, class_offsets[1])\n","edges_chr3L, subgraph_indices_chr3L = extract_edges_and_indices(adjacency_matrix_chr3L, class_offsets[2])\n","edges_chr3R, subgraph_indices_chr3R = extract_edges_and_indices(adjacency_matrix_chr3R, class_offsets[3])\n","\n","# Combine all edges and subgraph indices\n","all_edges = edges_chr2L + edges_chr2R + edges_chr3L + edges_chr3R\n","all_subgraph_indices = subgraph_indices_chr2L + subgraph_indices_chr2R + subgraph_indices_chr3L + subgraph_indices_chr3R\n","\n","# Save edges and subgraph indices to files\n","edges_file = '/content/edges.npy'\n","subgraph_indices_file = '/content/subgraph_indices.npy'\n","\n","np.save(edges_file, all_edges)\n","np.save(subgraph_indices_file, all_subgraph_indices)\n","\n","# Load edges and subgraph indices from the files to verify\n","loaded_edges = np.load(edges_file, allow_pickle=True)\n","loaded_subgraph_indices = np.load(subgraph_indices_file, allow_pickle=True)\n","\n","print(f\"Total edges loaded: {len(loaded_edges)}\")\n","print(f\"Total subgraph indices loaded: {len(loaded_subgraph_indices)}\")\n","\n","print(f\"First 10 subgraph indices: {loaded_subgraph_indices[:10]}\")\n","print(f\"Last 10 subgraph indices: {loaded_subgraph_indices[-10:]}\")\n","\n"],"metadata":{"id":"90UOBysrjcfH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Part 2e: class of subgraphs\n","import numpy as np\n","\n","num_subgraphs_per_class = 20000\n","num_classes = 4\n","subgraph_classes = np.concatenate([np.full(num_subgraphs_per_class, i) for i in range(num_classes)])\n","\n","subgraph_classes_file = '/content/subgraph_classes.npy'\n","\n","np.save(subgraph_classes_file, subgraph_classes)\n","\n","# Load subgraph classes from the file to verify\n","loaded_subgraph_classes = np.load(subgraph_classes_file, allow_pickle=True)\n","\n","print(f\"Shape of result: {loaded_subgraph_classes.shape}\")\n","\n","print(f\"First 10 subgraph classes: {loaded_subgraph_classes[:10]}\")\n","print(f\"Last 10 subgraph classes: {loaded_subgraph_classes[-10:]}\")\n","\n","import numpy as np\n","\n","# 假设 `task_2d` 已经包含了每条边所属的子图索引\n","task_2d_file = 'subgraph_indices.npy'\n","task_2d = np.load(task_2d_file, allow_pickle=True)\n","\n","# 定义子图的每个类包含的子图数量和类的数量\n","num_subgraphs_per_class = 20000\n","num_classes = 4\n","\n","# 根据子图索引生成每条边的类标签\n","task_2e = (task_2d // num_subgraphs_per_class).flatten()\n","\n","# 保存 `task_2e` 到文件\n","task_2e_file = 'task_2e.npy'\n","np.save(task_2e_file, task_2e)\n","\n","# 验证保存的文件\n","loaded_task_2e = np.load(task_2e_file, allow_pickle=True)\n","\n","print(\"Loaded Task 2E: \")\n","print(loaded_task_2e)\n","print(\"Shape of loaded Task 2E: \")\n","print(loaded_task_2e.shape)\n","print(\"First 10 edge classes: \", loaded_task_2e[:10])\n","print(\"Last 10 edge classes: \", loaded_task_2e[-10:])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzbnpLoIw2jw","executionInfo":{"status":"ok","timestamp":1718941933751,"user_tz":300,"elapsed":170,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"outputId":"ae411bae-a647-4acc-f7ba-bf2d5d120784"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of result: (80000,)\n","First 10 subgraph classes: [0 0 0 0 0 0 0 0 0 0]\n","Last 10 subgraph classes: [3 3 3 3 3 3 3 3 3 3]\n","Loaded Task 2E: \n","[0 0 0 ... 0 0 0]\n","Shape of loaded Task 2E: \n","(2875366,)\n","First 10 edge classes:  [0 0 0 0 0 0 0 0 0 0]\n","Last 10 edge classes:  [0 0 0 0 0 0 0 0 0 0]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29940,"status":"ok","timestamp":1718418311002,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"},"user_tz":300},"id":"vnryOZm5DrQd","outputId":"10ebb3b4-5673-4dc3-e486-8fecd5a5f4c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training set   = 64000 graphs\n","Validation set = 8000 graphs\n","Test set       = 8000 graphs\n","\n","Train loader:\n"," - Subgraph 0:\n","  Node labels (x): tensor([[10410],\n","        [ 7804],\n","        [  532],\n","        ...,\n","        [12565],\n","        [25404],\n","        [ 3285]])\n","  Labels (y): tensor([2, 2, 0, 1, 3, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 3, 0, 2, 0, 1, 0, 2, 0,\n","        1, 1, 2, 2, 3, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 3, 0, 1, 3,\n","        1, 1, 0, 2, 3, 2, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1])\n","\n","Validation loader:\n"," - Subgraph 0:\n","  Node labels (x): tensor([[29668],\n","        [29655],\n","        [51182],\n","        ...,\n","        [14097],\n","        [13295],\n","        [14097]])\n","  Labels (y): tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n","\n","Test loader:\n"," - Subgraph 0:\n","  Node labels (x): tensor([[51678],\n","        [42148],\n","        [19282],\n","        ...,\n","        [14783],\n","        [32275],\n","        [  460]])\n","  Labels (y): tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}],"source":["import torch\n","from torch_geometric.data import Data, InMemoryDataset, DataLoader\n","from torch_geometric.utils import from_scipy_sparse_matrix\n","import scipy.sparse as sp\n","import numpy as np\n","\n","def create_graph_data(adjacency_matrix, node_labels, label_number):\n","    graphs = []\n","    for i in range(adjacency_matrix.shape[0]):\n","        row = adjacency_matrix[i]\n","        labels = node_labels[i]\n","        assert len(row) == 21 * 21 + 1, f\"Each row should have 21*21 + 1 elements, but got {len(row)} elements.\"\n","        row = row[:-1]  # remove -1 row\n","        assert len(row) == 21 * 21, f\"Each row should have 21*21 elements after removing -1 column, but got {len(row)} elements.\"\n","        adj_matrix = row.reshape((21, 21))\n","        edge_index, edge_attr = from_scipy_sparse_matrix(sp.coo_matrix(adj_matrix))\n","        x = torch.tensor(labels, dtype=torch.long).view(-1, 1)  # Change to long tensor\n","        y = torch.tensor([label_number], dtype=torch.long)\n","        graph_data = Data(x=x, edge_index=edge_index, y=y)\n","        graphs.append(graph_data)\n","    return graphs\n","\n","worklist_node_labels = [node_labels_chr2L, node_labels_chr2R, node_labels_chr3L, node_labels_chr3R]\n","worklist_adjacency_matrix = [adjacency_matrix_chr2L, adjacency_matrix_chr2R, adjacency_matrix_chr3L, adjacency_matrix_chr3R]\n","\n","dataset = []\n","for label_number in range(len(worklist_adjacency_matrix)):\n","    adj_matrix = worklist_adjacency_matrix[label_number]\n","    node_labels = worklist_node_labels[label_number]\n","    graphs = create_graph_data(adj_matrix, node_labels, label_number)\n","    dataset.extend(graphs)\n","\n","class MyDataset(InMemoryDataset):\n","    def __init__(self, data_list, transform=None, pre_transform=None):\n","        self.data_list = data_list\n","        super(MyDataset, self).__init__(None, transform, pre_transform)\n","        self.data, self.slices = self.collate(data_list)\n","\n","    def len(self):\n","        return len(self.data_list)\n","\n","    def get(self, idx):\n","        return self.data_list[idx]\n","\n","my_dataset = MyDataset(dataset)\n","\n","num_node_features = my_dataset[0].x.size(1)\n","num_classes = len(set([data.y.item() for data in my_dataset]))\n","\n","train_dataset = my_dataset[:int(len(my_dataset) * 0.8)]\n","val_dataset = my_dataset[int(len(my_dataset) * 0.8):int(len(my_dataset) * 0.9)]\n","test_dataset = my_dataset[int(len(my_dataset) * 0.9):]\n","\n","print(f'Training set   = {len(train_dataset)} graphs')\n","print(f'Validation set = {len(val_dataset)} graphs')\n","print(f'Test set       = {len(test_dataset)} graphs')\n","\n","# Create mini-batches\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Verify if everything works properly\n","print('\\nTrain loader:')\n","for i, subgraph in enumerate(train_loader):\n","    print(f' - Subgraph {i}:')\n","    print(f'  Node labels (x): {subgraph.x}')\n","    print(f'  Labels (y): {subgraph.y}')\n","    break  # Print only the first batch\n","\n","print('\\nValidation loader:')\n","for i, subgraph in enumerate(val_loader):\n","    print(f' - Subgraph {i}:')\n","    print(f'  Node labels (x): {subgraph.x}')\n","    print(f'  Labels (y): {subgraph.y}')\n","    break  # Print only the first batch\n","\n","print('\\nTest loader:')\n","for i, subgraph in enumerate(test_loader):\n","    print(f' - Subgraph {i}:')\n","    print(f'  Node labels (x): {subgraph.x}')\n","    print(f'  Labels (y): {subgraph.y}')\n","    break  # Print only the first batch\n","\n","my_dataset = MyDataset(dataset)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3_b4rkxD9ic"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n","from torch_geometric.nn import GCNConv, GINConv\n","from torch_geometric.nn import global_mean_pool, global_add_pool\n","\n","\n","# class GCN(torch.nn.Module):\n","#     \"\"\"GCN\"\"\"\n","#     def __init__(self, dim_h):\n","#         super(GCN, self).__init__()\n","#         self.conv1 = GCNConv(dataset.num_node_features, dim_h)\n","#         self.conv2 = GCNConv(dim_h, dim_h)\n","#         self.conv3 = GCNConv(dim_h, dim_h)\n","#         self.lin = Linear(dim_h, dataset.num_classes)\n","\n","#     def forward(self, x, edge_index, batch):\n","#         # Node embeddings\n","#         h = self.conv1(x, edge_index)\n","#         h = h.relu()\n","#         h = self.conv2(h, edge_index)\n","#         h = h.relu()\n","#         h = self.conv3(h, edge_index)\n","\n","#         # Graph-level readout\n","#         hG = global_mean_pool(h, batch)\n","\n","#         # Classifier\n","#         h = F.dropout(hG, p=0.5, training=self.training)\n","#         h = self.lin(h)\n","\n","#         return hG, F.log_softmax(h, dim=1)\n","\n","class GIN(torch.nn.Module):\n","    \"\"\"GIN\"\"\"\n","    def __init__(self, dim_h):\n","        super(GIN, self).__init__()\n","        self.conv1 = GINConv(\n","            Sequential(Linear(my_dataset.num_node_features, dim_h),\n","                       BatchNorm1d(dim_h), ReLU(),\n","                       Linear(dim_h, dim_h), ReLU()))\n","        self.conv2 = GINConv(\n","            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n","                       Linear(dim_h, dim_h), ReLU()))\n","        self.conv3 = GINConv(\n","            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n","                       Linear(dim_h, dim_h), ReLU()))\n","        self.lin1 = Linear(dim_h*3, dim_h*3)\n","        self.lin2 = Linear(dim_h*3, my_dataset.num_classes)\n","\n","    def forward(self, x, edge_index, batch):\n","        # Node embeddings\n","        h1 = self.conv1(x, edge_index)\n","        h2 = self.conv2(h1, edge_index)\n","        h3 = self.conv3(h2, edge_index)\n","\n","        # Graph-level readout\n","        h1 = global_add_pool(h1, batch)\n","        h2 = global_add_pool(h2, batch)\n","        h3 = global_add_pool(h3, batch)\n","\n","        # Concatenate graph embeddings\n","        h = torch.cat((h1, h2, h3), dim=1)\n","\n","        # Classifier\n","        h = self.lin1(h)\n","        h = h.relu()\n","        h = F.dropout(h, p=0.5, training=self.training)\n","        h = self.lin2(h)\n","\n","        return h, F.log_softmax(h, dim=1)\n","\n","gin = GIN(dim_h=32)"]},{"cell_type":"code","source":["def train(model, loader):\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(),\n","                                      lr=0.01,\n","                                      weight_decay=0.01)\n","    epochs = 5\n","\n","    model.train()\n","    for epoch in range(epochs+1):\n","        #print(epoch)\n","        total_loss = 0\n","        acc = 0\n","        val_loss = 0\n","        val_acc = 0\n","\n","        # Train on batches\n","        for data in loader:\n","          optimizer.zero_grad()\n","          _, out = model(data.x, data.edge_index, data.batch)\n","          loss = criterion(out, data.y)\n","          total_loss += loss / len(loader)\n","          acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n","          loss.backward()\n","          optimizer.step()\n","\n","          # Validation\n","          val_loss, val_acc = test(model, val_loader)\n","\n","          # print per epoch to see if the accuracy goes up!\n","        print(f'Epoch {epoch:>0} | Train Loss: {total_loss:.2f} '\n","            f'| Train Acc: {acc*100:>5.2f}% '\n","            f'| Val Loss: {val_loss:.2f} '\n","            f'| Val Acc: {val_acc*100:.2f}%')\n","\n","\n","    test_loss, test_acc = test(model, test_loader)\n","    print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')\n","\n","    return model\n","\n","@torch.no_grad()\n","def test(model, loader):\n","    criterion = torch.nn.CrossEntropyLoss()\n","    model.eval()\n","    loss = 0\n","    acc = 0\n","\n","    for data in loader:\n","        _, out = model(data.x, data.edge_index, data.batch)\n","        loss += criterion(out, data.y) / len(loader)\n","        acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n","\n","    return loss, acc\n","\n","def accuracy(pred_y, y):\n","    \"\"\"Calculate accuracy.\"\"\"\n","    return ((pred_y == y).sum() / len(y)).item()\n","\n","gin = train(gin, train_loader)"],"metadata":{"id":"hBaabPB3DpSD","executionInfo":{"status":"ok","timestamp":1718411557036,"user_tz":300,"elapsed":8040665,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5dade58-a219-4686-dafd-71c309e4bd7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 | Train Loss: 1.18 | Train Acc: 43.08% | Val Loss: 2.09 | Val Acc: 18.74%\n","Epoch 1 | Train Loss: 1.11 | Train Acc: 45.85% | Val Loss: 1.88 | Val Acc: 27.40%\n","Epoch 2 | Train Loss: 1.11 | Train Acc: 45.74% | Val Loss: 1.47 | Val Acc: 40.22%\n","Epoch 3 | Train Loss: 1.10 | Train Acc: 46.32% | Val Loss: 1.57 | Val Acc: 34.62%\n","Epoch 4 | Train Loss: 1.10 | Train Acc: 46.59% | Val Loss: 1.86 | Val Acc: 31.59%\n","Epoch 5 | Train Loss: 1.10 | Train Acc: 46.53% | Val Loss: 1.35 | Val Acc: 49.36%\n","Test Loss: 1.33 | Test Acc: 50.40%\n"]}]},{"cell_type":"code","source":["\n","label_mapping = {0: '2L', 1: '2R', 2: '3L', 3: '3R'}\n","\n","\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","from torch_geometric.utils import to_networkx\n","\n","\n","fig, ax = plt.subplots(4, 4, figsize=(16, 16))\n","fig.suptitle('GIN - Graph classification')\n","\n","for i, data in enumerate(dataset[1113-16:]):\n","    _, out = gin(data.x, data.edge_index, data.batch)\n","    predicted_class = out.argmax(dim=1).item()\n","    actual_class = data.y.item()\n","    color = \"green\" if predicted_class == actual_class else \"red\"\n","\n","    ix = np.unravel_index(i, ax.shape)\n","    ax[ix].axis('off')\n","    G = to_networkx(dataset[1113-16+i], to_undirected=True)\n","    nx.draw_networkx(G,\n","                     pos=nx.spring_layout(G, seed=0),\n","                     with_labels=False,\n","                     node_size=150,\n","                     node_color=color,\n","                     width=0.8,\n","                     ax=ax[ix])\n","\n","    classification_text = f\"Predicted: {label_mapping[predicted_class]}, Actual: {label_mapping[actual_class]}\"\n","    ax[ix].text(0.5, 0.9, classification_text, horizontalalignment='center', verticalalignment='center', transform=ax[ix].transAxes, color='black')\n","\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"ap_XLaf0F7zQ","executionInfo":{"status":"error","timestamp":1717947337833,"user_tz":300,"elapsed":131,"user":{"displayName":"蔣遲衡","userId":"02891016855458749731"}},"outputId":"99ab29f5-80d8-4831-b553-16da463ad38c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch_geometric'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b3663345b372>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrUxeZwrhmFVgS2vNwzx7r"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}